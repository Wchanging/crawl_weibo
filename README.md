# 简单爬虫工具

一个为实验室项目服务的多平台数据爬取工具集（并不完整），支持部分主流社交媒体平台的内容和评论爬取。

## 功能概览

| 平台 | 可爬取帖子 | 可爬取评论 | 使用基础方法 | 使用外部API | 数据简单清洗 |
|------|-----------|-----------|-------------|-------------|-------------|
| 微博 | ✅ | ✅ | ✅ | ❌ | ✅ |
| 知乎 | ✅ | ✅ | ✅ | ❌ | ✅ |
| 小红书 | ❌ | ❌ | ❌ | ❌ | ✅ |
| 抖音 | ✅ | ❌ | ❌ | ✅ | ✅ |
| 微信公众号 | ✅ | ✅ | ❌ | ✅ | ✅ |

## 技术方案

### 基础方法
使用 `selenium` 和 `requests` 等常规爬虫库，通过模拟浏览器行为和API请求获取数据。

### 外部API
主要使用 [TikHub API](https://tikhub.io/) 服务，提供稳定的数据接口支持。

## 项目结构

```
├── weibo/          # 微博爬虫模块
├── zhihu/          # 知乎爬虫模块  
├── xhs/            # 小红书爬虫模块
├── douyin/         # 抖音爬虫模块
└── weixin/         # 微信公众号爬虫模块
```

## 主要功能

### 数据爬取
- **关键词搜索**: 支持基于关键词的内容搜索和爬取
- **帖子详情**: 获取帖子的详细内容、作者信息、发布时间等
- **评论数据**: 爬取帖子下的评论及回复信息

### 数据处理
- **数据清洗**: 去重、格式化、数据验证
- **数据合并**: 多个数据源的整合处理  
- **格式转换**: 支持JSON、CSV等多种输出格式
- **时间标准化**: 统一时间戳格式处理

## 环境配置

1. 安装依赖包：

```bash
pip install selenium requests pandas beautifulsoup4 python-dotenv
```

2. 配置环境变量（创建 `.env` 文件）：

```
TIKHUB_API_KEY=your_api_key_here
```

3. 下载对应浏览器驱动（如需要使用selenium）

## 使用说明

基础方法一般包含以下核心文件：

- `crawl_keywords.py` - 关键词搜索，基于selenium，一般最先使用以获取url
- `crawl_body.py` - 帖子内容爬虫  
- `crawl_comments.py` - 评论爬虫
- `clean_data.py` - 数据清洗工具
- `**_cookie.json` - requests需要的一些参数，不同网站的按名称替换**，内容可参考`weibo_cookie_sample.json`

调取api的方法建议自行前往对应网页探索

具体使用方法请参考各模块内的示例代码。

## 注意事项

- 请遵守各平台的robots.txt和使用条款
- 合理控制爬取频率，避免对服务器造成压力
- 仅用于学术研究和实验室项目
- 使用外部API时注意配额限制

## 免责声明

本工具的作者水平较低，不建议使用😭

本工具仅供学术研究使用，使用者需自行承担使用责任，确保符合相关法律法规和平台使用条款😁